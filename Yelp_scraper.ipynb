{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: fake_useragent in /opt/anaconda3/lib/python3.12/site-packages (2.0.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager fake_useragent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the URL stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.yelp.com/biz/mix-kitchen-and-bar-ithaca-11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "yelp_url = \"https://www.yelp.com/biz/mix-kitchen-and-bar-ithaca-11?osq=Reservations\"\n",
    "def extract_business_slug(yelp_url):\n",
    "    \"\"\"Extracts the business slug from a Yelp URL\"\"\"\n",
    "    parsed_url = urlparse(yelp_url)\n",
    "    path_parts = parsed_url.path.split(\"/\")\n",
    "    \n",
    "    if len(path_parts) > 2 and path_parts[1] == \"biz\":\n",
    "        business_slug = path_parts[2]  \n",
    "        clean_url = urljoin(\"https://www.yelp.com\", f\"/biz/{business_slug}\")  \n",
    "    return clean_url\n",
    "\n",
    "clean_yelp_url = extract_business_slug(yelp_url)\n",
    "clean_yelp_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "headers = {\"User-Agent\": ua.random}\n",
    "page = requests.get(clean_yelp_url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review[]\n",
      "Found 0 reviews.\n",
      "No reviews found on this page.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random \n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "headers = {\"User-Agent\": ua.random}\n",
    "def get_driver():\n",
    "    \"\"\"This function is used to set up a Selenium WebDriver with headless Chrome in order to go through the data within Yelp.\"\"\"\n",
    "    options = Options() \n",
    "    options.add_argument(\"--headless\")   \n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")     \n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    time.sleep(5)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def click_next_page(driver):\n",
    "    \"\"\"Clicks the 'Next' button to load the next page of reviews, if available.\"\"\"\n",
    "    try:\n",
    "        time.sleep(2)  # <--- Short delay to allow page update\n",
    "\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(@class, 'next-link')]\")\n",
    "\n",
    "        if next_button:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)  # Scroll to button\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)  \n",
    "            print(\"Clicked 'Next' button, loading next page...\")\n",
    "            time.sleep(4)  \n",
    "            return True  \n",
    "\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"No more pages or 'Next' button not found.\")\n",
    "        return False  \n",
    "\n",
    "    return False\n",
    "\n",
    "def scrape_yelp_reviews(clean_yelp_url):\n",
    "    \"This function extracts each yelp rating using Selenium\"\n",
    "    time.sleep(random.uniform(7, 14)) # <-- allows time for javascript to load\n",
    "    driver = get_driver()\n",
    "    driver.get(clean_yelp_url)\n",
    "    all_reviews = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        review_sections = driver.find_elements(By.XPATH, \"//ul[contains(@class, 'list__09f24__ynIEd')]//li\") # <--- tells the webscraper to start scraping at the start constumer reviews section\n",
    "        print(\"review\" + str(review_sections))\n",
    "        time.sleep(random.uniform(3, 7))\n",
    "        print(f\"Found {len(review_sections)} reviews.\")  # Debugging\n",
    "        if not review_sections:\n",
    "            print(\"No reviews found on this page.\")\n",
    "            break \n",
    "        \n",
    "        page_reviews = []\n",
    "        for review in review_sections:\n",
    "            try:\n",
    "                #extract aspects of review\n",
    "                reviewer_name = review.find_element(By.XPATH, \".//a[contains(@class, 'css-1lx1e1r')]\").text\n",
    "                reviewer_location = review.find_element(By.XPATH, \".//span[contains(@class, 'css-qgunke')]\").text\n",
    "                rating_element = review.find_elements(By.XPATH, \".//div[contains(@class, 'y-css-dnttlc')]\")  \n",
    "                if rating_element:\n",
    "                    rating = rating_element.get_attribute(\"aria-label\")  \n",
    "                else: \n",
    "                    continue\n",
    "                review_text = review.find_element(By.XPATH, \".//p[contains(@class, 'raw__09f24__T4Ezm')]\").text\n",
    "                review_date = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-1d8mpv1')]\").text\n",
    "                \n",
    "                #counts \n",
    "                helpful = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Helpfuk']/following-sibling::span[contains(@class, 'y-css-7nl72w')]\").text\n",
    "                thanks = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Thanks']/following-sibling::span[contains(@class, 'y-css-7nl72w')]\").text\n",
    "                love = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Love this']/following-sibling::span[contains(@class, 'y-css-7nl72w')]\").text\n",
    "                oh_no = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Oh no']/following-sibling::span[contains(@class, 'y-css-7nl72w')]\").text\n",
    "                \n",
    "                # Store all extracted data\n",
    "                review_data = {\n",
    "                    \"Reviewer Name\": reviewer_name,\n",
    "                    \"Location\": reviewer_location,\n",
    "                    \"Star Rating\": rating,\n",
    "                    \"Review Text\": review_text,\n",
    "                    \"Date\": review_date,\n",
    "                    \"Helpful\": helpful,\n",
    "                    \"Thanks\": thanks,\n",
    "                    \"Love\": love,\n",
    "                    \"Oh No\": oh_no\n",
    "                }\n",
    "                page_reviews.append(review_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting rating: {e}\") \n",
    "        time.sleep(2)\n",
    "        all_reviews.extend(page_reviews)  # <--- use extend since we are working with a dictionary\n",
    "        print(f\"Stored {len(review_data)} reviews from Page {page}.\")\n",
    "        if page > 2:\n",
    "            print(f\"Reached page limit {page}\")\n",
    "            break\n",
    "        if not click_next_page(driver):\n",
    "            print(\"No more pages to scrape reviews from\")\n",
    "            break\n",
    "        page += 1\n",
    "    driver.quit()\n",
    "    return all_reviews\n",
    "df_reviews = scrape_yelp_reviews(clean_yelp_url)\n",
    "print(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yelp_reviews.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def create_csv(df_reviews, file_path=\"yelp_reviews.csv\"):\n",
    "    \"\"\"This function converts df_reviews into a csv file\"\"\"\n",
    "    df = pd.DataFrame(df_reviews)\n",
    "    \n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    df.to_csv(file_path, mode='a', index=False, header=not pd.io.common.file_exists(file_path))\n",
    "\n",
    "    return file_path\n",
    "\n",
    "create_csv(df_reviews, file_path=\"yelp_reviews.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
