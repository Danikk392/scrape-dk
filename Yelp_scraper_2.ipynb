{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: fake_useragent in /opt/anaconda3/lib/python3.12/site-packages (2.0.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager fake_useragent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    ua = UserAgent()\n",
    "    return {\"User-Agent\": ua.random}\n",
    "\n",
    "# ✅ Use Proxies (Optional - Add your own proxies here)\n",
    "PROXIES = [\n",
    "    \"http://138.128.91.65:8000\",\n",
    "    \"http://192.252.208.67:14282\",\n",
    "    \"http://185.199.229.156:7492\"\n",
    "]\n",
    "\n",
    "def get_random_proxy():\n",
    "    return random.choice(PROXIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.yelp.com/biz/mix-kitchen-and-bar-ithaca-11'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "yelp_url = \"https://www.yelp.com/biz/mix-kitchen-and-bar-ithaca-11?osq=Reservations\"\n",
    "def extract_business_slug(yelp_url):\n",
    "    \"\"\"Extracts the business slug from a Yelp URL\"\"\"\n",
    "    parsed_url = urlparse(yelp_url)\n",
    "    path_parts = parsed_url.path.split(\"/\")\n",
    "    \n",
    "    if len(path_parts) > 2 and path_parts[1] == \"biz\":\n",
    "        business_slug = path_parts[2]  \n",
    "        clean_url = urljoin(\"https://www.yelp.com\", f\"/biz/{business_slug}\")  \n",
    "    return clean_url\n",
    "\n",
    "clean_yelp_url = extract_business_slug(yelp_url)\n",
    "clean_yelp_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(clean_yelp_url, max_retries=3):\n",
    "    session = requests.Session()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            proxy = {\"http\": get_random_proxy(), \"https\": get_random_proxy()}  # Rotate proxies\n",
    "            response = session.get(clean_yelp_url, headers=headers, proxies=proxy, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            \n",
    "            elif response.status_code in [403, 429]:\n",
    "                print(f\"Blocked! Changing User-Agent & Proxy... ({attempt+1}/{max_retries})\")\n",
    "                time.sleep(random.uniform(5, 10))  # Sleep before retrying\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "\n",
    "    print(\"Max retries reached. Exiting...\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  \n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")       \n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\")  # Fake user-agent\n",
    "    time.sleep(random.uniform(10, 20))  # Increased wait time\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_next_page(driver):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(@class, 'next')]\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(random.uniform(5, 10))  # <---- Add delay\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    for _ in range(3):  \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(5, 10))  \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  \n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 reviews on page 1.\n",
      "No reviews found on this page.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def scrape_yelp_reviews(clean_yelp_url):\n",
    "    time.sleep(random.uniform(7, 14))\n",
    "    driver = get_driver()  # Initialize WebDriver\n",
    "    driver.get(clean_yelp_url)\n",
    "    \n",
    "    all_reviews = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        time.sleep(random.uniform(4, 15))  # Random delay to avoid detection\n",
    "\n",
    "        review_sections = driver.find_elements(By.XPATH, \"//ul[@class= 'list__09f24__ynIEd']//li\")\n",
    "        print(f\"Found {len(review_sections)} reviews on page {page}.\")\n",
    "\n",
    "        if not review_sections:\n",
    "            print(\"No reviews found on this page.\")\n",
    "            break \n",
    "        individual_reviews = review_sections.find_elements(By.XPATH, \"//ul[@class= 'list__09f24__ynIEd']//li\")\n",
    "        \n",
    "        page_reviews = []\n",
    "        for review in individual_reviews:\n",
    "            try:\n",
    "                reviewer_name = review.find_element(By.XPATH, \".//a[contains(@class, 'css-1lx1e1r')]\").text\n",
    "                reviewer_location = review.find_element(By.XPATH, \".//span[contains(@class, 'css-qgunke')]\").text\n",
    "                \n",
    "                rating_element = review.find_elements(By.XPATH, \".//div[contains(@class, 'y-css-dnttlc')]\")\n",
    "                if rating_element:\n",
    "                    rating = rating_element[0].get_attribute(\"aria-label\")  \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                review_text = review.find_element(By.XPATH, \".//p[contains(@class, 'raw__09f24__T4Ezm')]\").text\n",
    "                review_date = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-1d8mpv1')]\").text\n",
    "\n",
    "                helpful = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Helpful']/following-sibling::span[contains(@class, 'y-css-7nL72w')]\").text\n",
    "                thanks = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Thanks']/following-sibling::span[contains(@class, 'y-css-7nL72w')]\").text\n",
    "                love = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Love this']/following-sibling::span[contains(@class, 'y-css-7nL72w')]\").text\n",
    "                oh_no = review.find_element(By.XPATH, \".//span[contains(@class, 'y-css-ghxju8') and text()='Oh no']/following-sibling::span[contains(@class, 'y-css-7nL72w')]\").text\n",
    "\n",
    "                review_data = {\n",
    "                    \"Reviewer Name\": reviewer_name,\n",
    "                    \"Location\": reviewer_location,\n",
    "                    \"Star Rating\": rating,\n",
    "                    \"Review Text\": review_text,\n",
    "                    \"Date\": review_date,\n",
    "                    \"Helpful\": helpful,\n",
    "                    \"Thanks\": thanks,\n",
    "                    \"Love\": love,\n",
    "                    \"Oh No\": oh_no\n",
    "                }\n",
    "                page_reviews.append(review_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting review: {e}\") \n",
    "        \n",
    "        all_reviews.extend(page_reviews)  \n",
    "        print(f\"Stored {len(page_reviews)} reviews from Page {page}.\")\n",
    "        \n",
    "        if page > 2:\n",
    "            print(f\"Reached page limit {page}\")\n",
    "            break\n",
    "\n",
    "        if not click_next_page(driver):\n",
    "            print(\"No more pages to scrape reviews from\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return all_reviews\n",
    "\n",
    "df_reviews = scrape_yelp_reviews(clean_yelp_url)\n",
    "print(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Run the scraper\u001b[39;00m\n\u001b[1;32m    116\u001b[0m clean_yelp_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.yelp.com/biz/sik-kitchen-and-bar-ithaca-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun(scrape_yelp_reviews(clean_yelp_url))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def get_driver():\n",
    "    \"\"\"Set up Playwright browser with stealth settings to avoid detection.\"\"\"\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless=True)  # Run in headless mode for stealth\n",
    "    context = await browser.new_context(\n",
    "        user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        viewport={\"width\": 1920, \"height\": 1080}\n",
    "    )\n",
    "    page = await context.new_page()\n",
    "    return playwright, browser, page\n",
    "\n",
    "async def scrape_yelp_reviews(clean_yelp_url):\n",
    "    \"\"\"Scrapes Yelp reviews using Playwright while handling CAPTCHA & pagination.\"\"\"\n",
    "    playwright, browser, page = await get_driver()\n",
    "    await page.goto(clean_yelp_url, timeout=60000)\n",
    "    await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "    page_content = await page.content()\n",
    "    print(page_content[:1000])  # Print first 1000 characters of the page\n",
    "\n",
    "    all_reviews = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        await asyncio.sleep(random.uniform(2, 4))\n",
    "\n",
    "        review_sections = await page.query_selector_all(\"//section[contains(@aria-label, 'Recommended Reviews')]//ul[contains(@class, 'list__09f24__ynIEd')]/li\")\n",
    "        \n",
    "        if not review_sections:\n",
    "            print(\"No reviews found on this page.\")\n",
    "            break\n",
    "\n",
    "        print(f\" Found {len(review_sections)} reviews on Page {page_number}.\")\n",
    "        page_reviews = []\n",
    "\n",
    "        for review in review_sections:\n",
    "            try:\n",
    "                reviewer_name = await review.query_selector_eval(\".//h4[contains(@class, 'css-1l1q8er')]\", \"el => el.textContent\") or \"N/A\"\n",
    "                reviewer_location = await review.query_selector_eval(\".//span[contains(@class, 'css-qgunke')]\", \"el => el.textContent\") or \"N/A\"\n",
    "                rating_element = await review.query_selector(\".//div[contains(@class, 'y-css-dnttlc')]\")\n",
    "                rating = await rating_element.get_attribute(\"aria-label\") if rating_element else \"N/A\"\n",
    "                review_text = await review.query_selector_eval(\".//p[contains(@class, 'raw__09f24__T4Ezm')]\", \"el => el.textContent\") or \"N/A\"\n",
    "                review_date = await review.query_selector_eval(\".//span[contains(@class, 'y-css-1d8mpv1')]\", \"el => el.textContent\") or \"N/A\"\n",
    "\n",
    "                helpful = await review.query_selector_eval(\".//span[contains(@class, 'y-css-ghxju8') and text()='Helpful']/following-sibling::span\", \"el => el.textContent\") or \"0\"\n",
    "                thanks = await review.query_selector_eval(\".//span[contains(@class, 'y-css-ghxju8') and text()='Thanks']/following-sibling::span\", \"el => el.textContent\") or \"0\"\n",
    "                love = await review.query_selector_eval(\".//span[contains(@class, 'y-css-ghxju8') and text()='Love this']/following-sibling::span\", \"el => el.textContent\") or \"0\"\n",
    "                oh_no = await review.query_selector_eval(\".//span[contains(@class, 'y-css-ghxju8') and text()='Oh no']/following-sibling::span\", \"el => el.textContent\") or \"0\"\n",
    "\n",
    "                review_data = {\n",
    "                    \"Reviewer Name\": reviewer_name.strip(),\n",
    "                    \"Location\": reviewer_location.strip(),\n",
    "                    \"Star Rating\": rating.strip(),\n",
    "                    \"Review Text\": review_text.strip(),\n",
    "                    \"Review Date\": review_date.strip(),\n",
    "                    \"Helpful\": helpful.strip(),\n",
    "                    \"Thanks\": thanks.strip(),\n",
    "                    \"Love\": love.strip(),\n",
    "                    \"Oh No\": oh_no.strip(),\n",
    "                }\n",
    "                page_reviews.append(review_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error extracting review: {e}\")\n",
    "\n",
    "        all_reviews.extend(page_reviews)\n",
    "        print(f\"Stored {len(page_reviews)} reviews from Page {page_number}.\")\n",
    "\n",
    "        if page_number >= 2:\n",
    "            print(\"Reached page limit, stopping.\")\n",
    "            break\n",
    "\n",
    "        success = await click_next_page(page)\n",
    "        if not success:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "    await browser.close()\n",
    "    return all_reviews\n",
    "\n",
    "async def click_next_page(page, max_retries=5):\n",
    "    \"\"\"Handles 'Next' button clicks with exponential backoff for 503 errors.\"\"\"\n",
    "    retries = 0\n",
    "    wait_time = 2\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            await asyncio.sleep(random.uniform(2, 3))\n",
    "            next_button = await page.query_selector(\"//a[contains(@class, 'next-link')]\")\n",
    "            if next_button:\n",
    "                await next_button.scroll_into_view_if_needed()\n",
    "                await asyncio.sleep(random.uniform(1, 2))\n",
    "                await next_button.click()\n",
    "                print(\"Clicked 'Next' button, loading next page...\")\n",
    "                await asyncio.sleep(random.uniform(4, 6))\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or 'Next' button not found: {e}\")\n",
    "            return False\n",
    "\n",
    "        retries += 1\n",
    "        print(f\"503 Error! Retrying in {wait_time} seconds... (Attempt {retries}/{max_retries})\")\n",
    "        await asyncio.sleep(wait_time)\n",
    "        wait_time *= 2\n",
    "\n",
    "    print(\"Max retries reached. Skipping page.\")\n",
    "    return False\n",
    "\n",
    "# Run the scraper\n",
    "clean_yelp_url = \"https://www.yelp.com/biz/sik-kitchen-and-bar-ithaca-3\"\n",
    "asyncio.run(scrape_yelp_reviews(clean_yelp_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
